{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Fundamentals\n",
    "## Group Project - Predicting the Outcome of a Car Accident in Zurich City\n",
    "#### Group Members:\n",
    "- Elia Locher (21-941-000)\n",
    "- Gabriel Oget (22-610-893)\n",
    "- Marco Molnar (21-917-315)\n",
    "#### Content:\n",
    "1. Feature Engineering and Data Preprocessing\n",
    "2. Data Exploration and Analysis\n",
    "3. Model Training\n",
    "4. Results and Concluion\n",
    "5. Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import shapely as shp\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import math\n",
    "from datetime import datetime, timedelta, date\n",
    "import seaborn as sns\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dateutil.easter import easter\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, KFold, cross_val_predict, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet, RidgeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, recall_score, precision_score, f1_score, roc_auc_score, roc_curve, auc, make_scorer, precision_recall_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from keras_tuner import RandomSearch, Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Engineering and Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importing the Main Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and only select the columns we need\n",
    "accidents_raw = pd.read_csv('roadtrafficaccidentlocations.csv', usecols=['AccidentUID', 'AccidentType', 'AccidentSeverityCategory', 'AccidentInvolvingPedestrian', 'AccidentInvolvingBicycle', 'AccidentInvolvingMotorcycle', 'RoadType', 'AccidentYear', 'AccidentMonth', 'AccidentWeekDay', 'AccidentHour', 'AccidentLocation_CHLV95_E', 'AccidentLocation_CHLV95_N'])\n",
    "# exclude 2011, because we don't have traffic volume data for that year\n",
    "accidents_raw = accidents_raw.loc[accidents_raw['AccidentYear'] >= 2012].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_raw.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Preprocessing: Date and Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further Feature Engineering it is useful if we have a Date-Time column. Given the year, month wnd weekday and assuming a chronological order of the observations, we can calculate the exact Date. The following function also assumes there was at least one accident per week, which seems plausable as there are 55000 accidents and only a total of about 4000 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the Weekday column\n",
    "accidents_raw['AccidentWeekDay'] = accidents_raw['AccidentWeekDay'].str[-1].astype(int)\n",
    "\n",
    "# Setting the start date\n",
    "year = accidents_raw['AccidentYear'][0] # 2011\n",
    "month = accidents_raw['AccidentMonth'][0] # 1\n",
    "# weekday is the information we have (1 to 7 for Monday to Sunday)\n",
    "weekday = accidents_raw['AccidentWeekDay'][0] # 6\n",
    "# monthday is the information we want to calculate (1 to 31 for the day of the month)\n",
    "monthday = accidents_raw['AccidentMonth'][0] # 1\n",
    "\n",
    "# Initialize the DateTime column\n",
    "accidents_raw['DateTime'] = datetime(1900, 1, 1, 0, 0, 0)\n",
    "\n",
    "# Iterate over all rows and calculate the DateTime\n",
    "for i in tqdm(accidents_raw.index):\n",
    "    # The hour will just stay the same\n",
    "    hour = accidents_raw['AccidentHour'][i]\n",
    "    \n",
    "    # Is the weekday the same as before?\n",
    "    if accidents_raw['AccidentWeekDay'][i] != weekday:\n",
    "        new_weekday = accidents_raw['AccidentWeekDay'][i]\n",
    "        # calculate the distance between the two weekdays and add it to the monthday\n",
    "        distance = new_weekday - weekday\n",
    "        if distance < 0:\n",
    "            distance += 7\n",
    "        monthday += distance\n",
    "        weekday = new_weekday\n",
    "        \n",
    "        # if new month has started, reset monthday and reload year and month, becase we only have to recheck the year if the month has changed\n",
    "        if accidents_raw['AccidentMonth'][i] != month:\n",
    "            monthday = 1\n",
    "            year = accidents_raw['AccidentYear'][i]\n",
    "            month = accidents_raw['AccidentMonth'][i]\n",
    "            \n",
    "    accidents_raw.loc[i, 'DateTime'] = datetime(year, month, monthday, hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_raw.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data Preprocessing: Coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also useful to have a column with preformatted coordinate points for each observation, replacing the 'E' and 'N' Coordinates. By the way: All our datasets that use geospacial information, use a specific swiss coordinate reference system, so we dont need to worry about different systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a list comprehension to create the geometry column with the shapely Point objects\n",
    "accidents_raw['geometry'] = [shp.Point(x, y) for x, y in zip(accidents_raw['AccidentLocation_CHLV95_E'], accidents_raw['AccidentLocation_CHLV95_N'])]\n",
    "# also create a GeoDataFrame and drop the old columns\n",
    "accidents_raw = gpd.GeoDataFrame(accidents_raw, geometry='geometry')#.drop(['AccidentLocation_CHLV95_E', 'AccidentLocation_CHLV95_N'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_raw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_raw['geometry'].plot(figsize=(30, 30), alpha=0.2, markersize=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Feature Engineering: Traffic Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first real feature we will implement here is the traffic volume, measured by automated counting stations all over Zurich City."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_data_types = {'HNr': str, 'D2ID': str}\n",
    "\n",
    "def read_traffic_data(year):\n",
    "    filename = f\"Datasets/Verkehrsaufkommen/sid_dav_verkehrszaehlung_miv_OD2031_{year}.csv\"\n",
    "    return pd.read_csv(filename, dtype=column_data_types, usecols=['MSID', 'MSName', 'ZSID', 'ZSName', 'EKoord', 'NKoord', 'MessungDatZeit', 'AnzFahrzeuge', 'Richtung'])\n",
    "\n",
    "trafficdic = {}\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = {executor.submit(read_traffic_data, i): i for i in range(2012, 2023)}\n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        year = futures[future]\n",
    "        trafficdic[year] = future.result()\n",
    "        \n",
    "traffic_coords = trafficdic[2022][['EKoord', 'NKoord']].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First look at the first traffic frequency dataframe\n",
    "trafficdic[2012].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our interesting variable 'AnzFahrzeuge' which is the measurement of the traffic frequency has a lot of missing values. Just look at the observation with missing values for the variable 'AnzFahrzeuge' below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafficdic[2012][trafficdic[2012][\"AnzFahrzeuge\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this reason we have to impute this variable. We will show two different methods for imputing the variable 'AnzFahrzeuge'.\n",
    "But before we are going to impoute the variable. We want to discuss how we want to build this dataframe up. The first point is that we are going to calculate the distance between each accident and the different counting stations. To compute the distance between two points p and q we will use the so called 'Euclidean distance':\n",
    "$$ d(p, q)^2 = (q_1 - p_1)^2 + (q_2 - p_2)^2 $$\n",
    "\n",
    "As a next step we are going to evaluate the six nearest counting stations, so we will calculate the six smallest distances. The notation is the following:\n",
    "+ 'CountingStation_MinDistance' is the station which measures traffic frequency, which is AnzFahrzeuge' is the traffic measurement from the nearest counting station to the accident\n",
    "+ 'AnzFahrzeuge_2thMinDistance' is the traffic measurement from the second nearest counting station to the accident and so on until the variable 'AnzFahrzeuge_6thMinDistance'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 Traffic: Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nth_smallest_info(group, n):\n",
    "    if len(group) >= n:\n",
    "        nth_smallest_row = group.nsmallest(n, 'Distanz').iloc[-1]\n",
    "        return pd.Series({\n",
    "            f'{n}thMinDistance': nth_smallest_row['Distanz'],\n",
    "            f'CountingStation_{n}thMinDistance': nth_smallest_row['CountingStation'],\n",
    "            f'AnzFahrzeuge_{n}thMinDistance': nth_smallest_row['AnzFahrzeuge'],\n",
    "            f'AnzFahrzeuge_{n}thMinDistance_-1h': nth_smallest_row['AnzFahrzeuge_-1h'],\n",
    "            f'AnzFahrzeuge_{n}thMinDistance_-2h': nth_smallest_row['AnzFahrzeuge_-2h']\n",
    "        })\n",
    "    return pd.Series({\n",
    "        f'{n}thMinDistance': None,\n",
    "        f'CountingStation_{n}thMinDistance': None,\n",
    "        f'AnzFahrzeuge_{n}thMinDistance': None,\n",
    "        f'AnzFahrzeuge_{n}thMinDistance_-1h': None,\n",
    "        f'AnzFahrzeuge_{n}thMinDistance_-2h': None\n",
    "    })\n",
    "    \n",
    "def prepare_traffic_data(df_traffic,year):\n",
    "    zurich = accidents_raw.copy()\n",
    "    zurich[\"Date\"] = accidents_raw[\"DateTime\"]\n",
    "    df_accident = zurich[zurich['AccidentYear'] == year][[\"Date\", \"AccidentUID\", \"AccidentLocation_CHLV95_E\", \"AccidentLocation_CHLV95_N\"]]\n",
    "\n",
    "    df_traffic[\"Date\"] = pd.to_datetime(df_traffic[\"MessungDatZeit\"])\n",
    "    df_traffic['Year'] = df_traffic['Date'].dt.year\n",
    "    df_traffic['Weekday'] = df_traffic['Date'].dt.weekday  # Monday is 0, Sunday is 6\n",
    "    df_traffic['Daytime'] = df_traffic['Date'].dt.time\n",
    "    df_traffic[\"CountingStation\"] = df_traffic[\"ZSName\"] + \" \" + df_traffic[\"Richtung\"]\n",
    "    df_traffic['AnzFahrzeuge_-1h'] = df_traffic.groupby('CountingStation')['AnzFahrzeuge'].shift(1)\n",
    "    df_traffic['AnzFahrzeuge_-2h'] = df_traffic.groupby('CountingStation')['AnzFahrzeuge'].shift(2)\n",
    "    \n",
    "    missings = df_traffic[df_traffic[\"AnzFahrzeuge\"].isna()].value_counts('CountingStation').reset_index()\n",
    " \n",
    "    # Schwellenwert berechnen (50% von 8760) --> 8760 = 365*24; Maximal number of observations per station and year\n",
    "    threshold = 0.50 * 8760\n",
    "\n",
    "    # Filtern des DataFrames\n",
    "    filtered_stations = missings[missings['count'] > threshold]\n",
    "\n",
    "    # Anzeigen der gefilterten 'CountingStation'\n",
    "    filtered_stations\n",
    "\n",
    "    stations_list = filtered_stations['CountingStation'].tolist()\n",
    "    df_traffic = df_traffic.loc[~df_traffic['CountingStation'].isin(stations_list)]\n",
    "    \n",
    "    # Gruppieren der Werte, sodass jede Gruppe eine Messung an einer CountingStation zu einer spezifischen Tageszeit an einem Wochentag darstellt  \n",
    "    # Die fehlenden Werte werden durch den (jahres) Gruppendurchschnitt imputiert\n",
    "    imputation = df_traffic.groupby(['CountingStation', 'Weekday', 'Daytime'])['AnzFahrzeuge'].mean().round(0).reset_index()\n",
    "    imputation.rename(columns={\"AnzFahrzeuge\":\"ImputedAnzFahrzeuge\"},inplace=True)\n",
    "    \n",
    "    df_traffic = pd.merge(df_traffic,imputation,on=['CountingStation','Weekday','Daytime'],how=\"left\")\n",
    "    df_traffic[\"AnzFahrzeuge\"] = df_traffic[\"AnzFahrzeuge\"].fillna(df_traffic[\"ImputedAnzFahrzeuge\"])\n",
    "    df_traffic = df_traffic.sort_values(by=[\"CountingStation\",\"Date\"])\n",
    "    df_traffic['AnzFahrzeuge_-1h'] = df_traffic.groupby('CountingStation')['AnzFahrzeuge'].shift(1)\n",
    "    df_traffic['AnzFahrzeuge_-2h'] = df_traffic.groupby('CountingStation')['AnzFahrzeuge'].shift(2)\n",
    "    df_traffic.drop(columns=[\"MSID\",\"MSName\",\"ZSID\",\"ZSName\",\"Richtung\",\"MessungDatZeit\",\"Weekday\",\"Daytime\"], inplace=True)\n",
    "    df_traffic['Year'] = df_traffic['Date'].dt.year\n",
    "    df_traffic = df_traffic[[\"Date\",\"Year\",\"EKoord\",\"NKoord\",\"CountingStation\",\"AnzFahrzeuge\",\"AnzFahrzeuge_-1h\",\"AnzFahrzeuge_-2h\"]]\n",
    "    \n",
    "    merged = pd.merge(df_traffic,df_accident,on=\"Date\",how=\"left\")\n",
    "    merged['Distanz'] = np.sqrt((merged['AccidentLocation_CHLV95_E'] - merged['EKoord'])**2 +\n",
    "                               (merged['AccidentLocation_CHLV95_N'] - merged['NKoord'])**2)\n",
    "    \n",
    "    min_info_per_accident = merged.groupby('AccidentUID').apply(lambda x: x.nsmallest(1, 'Distanz')).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    # Rename columns for the minimum distance and other related fields\n",
    "    min_info_per_accident.rename(columns={\n",
    "        'Distanz': 'MinDistance', \n",
    "        'CountingStation': 'CountingStation_MinDistance'}, inplace=True)\n",
    "\n",
    "    # Merge the min info back into the original dataframe\n",
    "    merged = merged.merge(min_info_per_accident[['AccidentUID', 'MinDistance', 'CountingStation_MinDistance']], on='AccidentUID')\n",
    "    merged = merged.sort_values(by=[\"CountingStation\",\"Date\"])\n",
    "    # Function to get the nth smallest distance and corresponding information for each AccidentUID\n",
    "    \n",
    "    # Applying the function for 2nd to 6th smallest distances\n",
    "    for i in range(2, 4):\n",
    "        nth_min_info = merged.groupby('AccidentUID').apply(lambda group: get_nth_smallest_info(group, i)).reset_index()\n",
    "        merged = merged.merge(nth_min_info, on='AccidentUID')\n",
    "\n",
    "    merged = merged[\n",
    "        (merged['Distanz'] == merged['MinDistance']) &\n",
    "        (merged['CountingStation'] == merged['CountingStation_MinDistance'])]\n",
    "    \n",
    "    columns_to_fill_withzero = ['AnzFahrzeuge_-1h','AnzFahrzeuge_2thMinDistance_-1h','AnzFahrzeuge_3thMinDistance_-1h',\n",
    "                                'AnzFahrzeuge_-2h','AnzFahrzeuge_2thMinDistance_-2h','AnzFahrzeuge_3thMinDistance_-2h']\n",
    "\n",
    "    # Durch die Liste iterieren und NaN-Werte mit 0 auffüllen\n",
    "    for col in columns_to_fill_withzero:\n",
    "        merged[col] = merged[col].fillna(0)\n",
    "\n",
    "    # to account for data leackage, we dont select the traffic volume as counted in the same hour the accident occurred in\n",
    "    df_imputed = merged[[\"Date\",\"AccidentUID\",\"CountingStation_MinDistance\",\"MinDistance\",\"AnzFahrzeuge_-1h\",\"AnzFahrzeuge_-2h\",\n",
    "                                  \"CountingStation_2thMinDistance\",\"2thMinDistance\",\"AnzFahrzeuge_2thMinDistance_-1h\",\"AnzFahrzeuge_2thMinDistance_-2h\",\n",
    "                                  \"CountingStation_3thMinDistance\",\"3thMinDistance\",\"AnzFahrzeuge_3thMinDistance_-1h\",\"AnzFahrzeuge_3thMinDistance_-2h\"\n",
    "    ]]\n",
    "    \n",
    "    return df_imputed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in tqdm(range(2012, 2023)):\n",
    "    trafficdic[y] = prepare_traffic_data(trafficdic[y],y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic = pd.concat(trafficdic.values(), ignore_index=True)\n",
    "accidents_raw = pd.merge(accidents_raw, traffic, on=['AccidentUID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_raw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "accidents_raw['geometry'].plot(ax=ax, alpha=0.2, markersize=4)\n",
    "ax.scatter(traffic_coords['EKoord'], traffic_coords['NKoord'], s=50, color='#4d004d', alpha=0.8, marker='D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Feature Engineering: Weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use the weather data collected by the three weather stations in Zurich to create lagged features for temperature, rainfall, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all Datasets and storing them in a dictionary\n",
    "weatherdic = {}\n",
    "\n",
    "def read_weather_data(year):\n",
    "    filename = f\"Datasets/Wetter/ugz_ogd_meteo_h1_{year}.csv\"\n",
    "    return pd.read_csv(filename, usecols=['Datum', 'Standort', 'Parameter', 'Wert'])\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = {executor.submit(read_weather_data, i): i for i in range(2011, 2023)}\n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        year = futures[future]\n",
    "        weatherdic[year] = future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherdic[2011].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All values of 'Status' are 'Bereinigt' for all years, meaning there is no skewed data due to for example technical failures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.1 Weather: Transforming Long to Wide Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that tranforms long DF into wide DF and format datetime\n",
    "def transformation (df):\n",
    "    # Transform Datum into str...\n",
    "    #df.Datum.astype('str')\n",
    "    # and then into datetime\n",
    "    df['DateTime'] = pd.to_datetime(df['Datum'])\n",
    "    df.drop(columns=[\"Datum\"], inplace=True)\n",
    "    \n",
    "    # Transformation from long to wide format\n",
    "    df = df.pivot(\n",
    "        # We drop Intervall, Einheit and Status, as they are consistent over all dataframes and parameters\n",
    "        index=[\"DateTime\", \"Standort\"], columns=[\"Parameter\"], values=\"Wert\").reset_index() #index ist der Identifier\n",
    "\n",
    "    return df #return the wide DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to all Dataframes\n",
    "for i in range(2011, 2023):\n",
    "    weatherdic[i] = transformation(weatherdic[i])\n",
    "\n",
    "# Concat all the wide Dataframes\n",
    "weather_wide = pd.concat([df for df in weatherdic.values()], ignore_index=True)\n",
    "\n",
    "# Rename the column Parameter by adding information about the Einheit (unit) of each Parameter\n",
    "weather_wide.rename(columns={'T': 'Temp (°C)', 'Hr': 'Hr (%Hr)', 'p': 'p (hPa)', 'RainDur': 'RainDur (min)', 'StrGlo': 'StrGlo (W/m2)',\n",
    "                             'WD': 'WD (°)', 'WVv': 'WVv (m/s)', 'WVs': 'WVs (m/s)'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_wide.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.2 Weather: Getting the Mean for each Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First look how many Weatherstations we have\n",
    "weather_wide['Standort'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create three separate DF's for each weatherstation\n",
    "rosengarten = weather_wide.query(\"Standort == 'Zch_Rosengartenstrasse'\")\n",
    "schimmel = weather_wide.query(\"Standort == 'Zch_Schimmelstrasse'\")\n",
    "stampfenbach = weather_wide.query(\"Standort == 'Zch_Stampfenbachstrasse'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the three DF's by date in order to have the hourly weather variable of each station at a given date in the same row\n",
    "weather_wide = rosengarten.merge(schimmel,on='DateTime', how ='outer').merge(stampfenbach,on='DateTime',how ='outer')#.drop(columns=['Standort_x','Standort_y','Standort'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the values by Date\n",
    "weather_wide.sort_values(by='DateTime', inplace=True)\n",
    "weather_wide = weather_wide.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate for each weather variable the row wise mean -> we get the mean of each weather variable from the three stations\n",
    "# by default the .mean()-method ignores missing values (e.g on 01.01.2012 0:00 we only have the weather variable of the weather stations\n",
    "# at Schimmelstrasse and Stampfenbachstrasse but at Rosengartenstrasse we have a NaN. The .mean()-methode therefore takes the value of\n",
    "# the Schimmelstrasse and Stampfenbachstrasse and divides it by two. As a result we get the correct mean of a weather variable at a given hour.)\n",
    "weather_wide['Hr (%Hr)_mean'] = weather_wide[['Hr (%Hr)_x', 'Hr (%Hr)_y', 'Hr (%Hr)']].mean(axis=1)\n",
    "weather_wide['RainDur (min)_mean'] = weather_wide[['RainDur (min)_x', 'RainDur (min)_y', 'RainDur (min)']].mean(axis=1)\n",
    "weather_wide['StrGlo (W/m2)_mean'] = weather_wide[['StrGlo (W/m2)_x', 'StrGlo (W/m2)_y', 'StrGlo (W/m2)']].mean(axis=1)\n",
    "weather_wide['Temp (°C)_mean'] = weather_wide[['Temp (°C)_x', 'Temp (°C)_y', 'Temp (°C)']].mean(axis=1).drop(columns=['Temp (°C)_x', 'Temp (°C)_y', 'Temp (°C)'])\n",
    "weather_wide['WD (°)_mean'] = weather_wide[['WD (°)_x', 'WD (°)_y', 'WD (°)']].mean(axis=1).drop(columns=['WD (°)_x', 'WD (°)_y', 'WD (°)'])\n",
    "weather_wide['WVs (m/s)_mean'] = weather_wide[['WVs (m/s)_x', 'WVs (m/s)_y', 'WVs (m/s)']].mean(axis=1)\n",
    "weather_wide['WVv (m/s)_mean'] = weather_wide[['WVv (m/s)_x', 'WVv (m/s)_y', 'WVv (m/s)']].mean(axis=1)\n",
    "weather_wide['p (hPa)_mean'] = weather_wide[['p (hPa)_x', 'p (hPa)_y', 'p (hPa)']].mean(axis=1)\n",
    "\n",
    "# as we are only interested in the mean of the weather variables, we can now drop the specific weather variables of each station\n",
    "weather_wide = weather_wide.drop(columns=['index', 'Hr (%Hr)_x', 'Hr (%Hr)_y', 'Hr (%Hr)', 'RainDur (min)_x', 'RainDur (min)_y', 'RainDur (min)', 'StrGlo (W/m2)_x', 'StrGlo (W/m2)_y', 'StrGlo (W/m2)', 'Temp (°C)_x', 'Temp (°C)_y', 'Temp (°C)', 'WD (°)_x', 'WD (°)_y', 'WD (°)', 'WVs (m/s)_x', 'WVs (m/s)_y', 'WVs (m/s)', 'WVv (m/s)_x', 'WVv (m/s)_y', 'WVv (m/s)', 'p (hPa)_x', 'p (hPa)_y', 'p (hPa)', 'Standort_x', 'Standort_y', 'Standort'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_wide.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.3 Weather: Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(weather_wide['StrGlo (W/m2)_mean'].isna() == True) # did this code cell after knowing about the NaN's (see bellow). We wanted to know\n",
    "# how the 143 missing values are distributed in the dataframe. Result: we have maximum 27h hours in series NaN's, followed by 21h and 9h in a\n",
    "# a row. This result gives information wether the imputation methode of the NaN's is acceptable or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_wide.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns 'Hr (%Hr)_mean', 'StrGlo (W/m2)_mean', 'Temp (°C)_mean', 'WVs (m/s)_mean', contain missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will drop the column WVs (m/s)_mean as we have to many missing values to select a representiv formula to impute the missing values\n",
    "weather_wide.drop('WVs (m/s)_mean',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.4 Weather: Creating Time Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the UTC +1 timezone information\n",
    "weather_wide['DateTime'] = pd.to_datetime(weather_wide['DateTime']).dt.tz_convert(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to add the value of each weather variable from the n-1st, n-2nd and n-3rd hour to the row of the n-th hour.\n",
    "\n",
    "# List of Wettervariablen\n",
    "wettervariablen = ['Hr (%Hr)_mean', 'RainDur (min)_mean', 'StrGlo (W/m2)_mean', 'Temp (°C)_mean', 'WD (°)_mean', 'p (hPa)_mean', 'WVv (m/s)_mean']\n",
    "\n",
    "weather_wide.sort_values(by='DateTime', inplace=True)\n",
    "\n",
    "# Iteriere über jede Wettervariable und füge die verschobenen Werte (um eine Stunde) hinzu\n",
    "for variable in wettervariablen:\n",
    "    weather_wide.loc[:, f'{variable}-1h'] = weather_wide.loc[:, variable].shift(1)\n",
    "# Iteriere über jede Wettervariable und füge die verschobenen Werte (um zwei Stunde) hinzu\n",
    "    weather_wide.loc[:, f'{variable}-2h'] = weather_wide.loc[:, variable].shift(2)\n",
    "# Iteriere über jede Wettervariable und füge die verschobenen Werte (um drei Stunde) hinzu\n",
    "    weather_wide.loc[:, f'{variable}-3h'] = weather_wide.loc[:, variable].shift(3)\n",
    "    \n",
    "weather_wide.drop(columns=wettervariablen, inplace=True) # drop the original columns to prevent data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_wide.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the exact dates of the daylightssavings time shift in a dictionary\n",
    "timeshiftdic = {\n",
    "    2012: [datetime(2012, 3, 25, 2, 0, 0), datetime(2012, 10, 28, 2, 0, 0)],\n",
    "    2013: [datetime(2013, 3, 31, 2, 0, 0), datetime(2013, 10, 27, 2, 0, 0)],\n",
    "    2014: [datetime(2014, 3, 30, 2, 0, 0), datetime(2014, 10, 26, 2, 0, 0)],\n",
    "    2015: [datetime(2015, 3, 29, 2, 0, 0), datetime(2015, 10, 25, 2, 0, 0)],\n",
    "    2016: [datetime(2016, 3, 27, 2, 0, 0), datetime(2016, 10, 30, 2, 0, 0)],\n",
    "    2017: [datetime(2017, 3, 26, 2, 0, 0), datetime(2017, 10, 29, 2, 0, 0)],\n",
    "    2018: [datetime(2018, 3, 25, 2, 0, 0), datetime(2018, 10, 28, 2, 0, 0)],\n",
    "    2019: [datetime(2019, 3, 31, 2, 0, 0), datetime(2019, 10, 27, 2, 0, 0)],\n",
    "    2020: [datetime(2020, 3, 29, 2, 0, 0), datetime(2020, 10, 25, 2, 0, 0)],\n",
    "    2021: [datetime(2021, 3, 28, 2, 0, 0), datetime(2021, 10, 31, 2, 0, 0)],\n",
    "    2022: [datetime(2022, 3, 27, 2, 0, 0), datetime(2022, 10, 30, 2, 0, 0)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over all years and shift the DateTime column by one hour for the daylight savings time\n",
    "for year in tqdm(timeshiftdic.keys()):\n",
    "    # delete the hour that would be duplicated by the shift\n",
    "    weather_wide.drop(weather_wide.loc[weather_wide['DateTime'] == timeshiftdic[year][1]].index, inplace=True)\n",
    "    weather_wide.loc[(weather_wide['DateTime'] >= timeshiftdic[year][0]) & (weather_wide['DateTime'] < timeshiftdic[year][1]), 'DateTime'] += timedelta(hours=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.5 Weather: Data Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found out, that in column 'Hr (%Hr)_mean', 'StrGlo (W/m2)_mean', 'Temp (°C)_mean' we have missing values. \n",
    "Therefore we replace these NaNs with the previous value of the weather_wide df (i.e. we replace the missing values with the values \n",
    "from the previous hour). Since the weather varies from day to day over the years, we find it most appropriate to use the previous hour's value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN-Values in the column 'Hr (%Hr)_mean' by the previous value\n",
    "weather_wide.ffill(inplace=True) # first forward fill all NaN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# controll check if there are no more missing values\n",
    "# first we remove 2011\n",
    "weather_wide = weather_wide[weather_wide['DateTime'].dt.year != 2011]\n",
    "# then we check if there are any missing values\n",
    "weather_wide.isna().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.6 Weather: Merging weather_wide with accidents_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, perform the merge\n",
    "accidents_raw = pd.merge(accidents_raw, weather_wide, how='left', on='DateTime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_raw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_coords = {'E': [2681943, 2683148, 2682106], 'N': [1247245, 1249020, 1249935]}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "accidents_raw['geometry'].plot(ax=ax, alpha=0.2, markersize=4)\n",
    "ax.scatter(traffic_coords['EKoord'], traffic_coords['NKoord'], s=50, color='#4d004d', alpha=0.8, marker='D')\n",
    "ax.scatter(weather_coords['E'], weather_coords['N'], s=150, alpha=0.8, color='green', marker='p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Feature Engineering: Public Transport Stops and Garages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the files we're gonna use\n",
    "haltestellen = pd.read_csv('Datasets/Haltestellen.csv', usecols=['SYMB_TEXT', 'E', 'N']) # shows all busstop's in the City of Zürich\n",
    "haltestellen_coords = haltestellen[['E', 'N']]\n",
    "\n",
    "garage = pd.read_csv('Datasets/stzh.poi_parkhaus_view.csv', usecols=['anzahl_oeffentliche_pp', 'geometry'])\n",
    "garage['geometry'] = garage['geometry'].apply(shp.wkt.loads)\n",
    "garage = gpd.GeoDataFrame(garage, geometry='geometry')\n",
    "\n",
    "garage_coords = garage[['geometry']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haltestellen.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garage.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haltestellen['SYMB_TEXT'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create features for the type of public transport stop, we take the SYMB_TEXT column and create boolean values, based on wether a given stop contains a certain vehicle in its string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haltestellen['Bus'] = haltestellen['SYMB_TEXT'].str.contains('Bus')\n",
    "haltestellen['Tram'] = haltestellen['SYMB_TEXT'].str.contains('Tram')\n",
    "haltestellen['Bahn'] = haltestellen['SYMB_TEXT'].str.contains('Bahn')\n",
    "haltestellen['Bergbahn'] = haltestellen['SYMB_TEXT'].str.contains('Bergbahn')\n",
    "haltestellen.drop(columns=['SYMB_TEXT'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some stops dont contain a value for SYMB_TEXT. These are mainly fairy stops at lake Zurich. We decided to leave them in and give the model the opportunity to find certain patterns relating to stops, where all vehicle booleans are False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haltestellen.fillna(False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haltestellen.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garage['anzahl_oeffentliche_pp'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a GeoDataFrame for spacial indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haltestellen['geometry'] = [shp.Point(x, y) for x, y in zip(haltestellen['E'], haltestellen['N'])]\n",
    "haltestellen = gpd.GeoDataFrame(haltestellen, geometry='geometry').drop(['E', 'N'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for indexU, accident_point in enumerate(tqdm(accidents_raw['geometry'])):\n",
    "    \n",
    "    # Find and assign closest parking garage and public transport stop\n",
    "    indexH = haltestellen.sindex.nearest(geometry=accident_point)[1][0]\n",
    "    accidents_raw.loc[indexU, 'PTS_id'] = indexH\n",
    "    accidents_raw.loc[indexU, 'distance_PTS'] = math.dist(accident_point.coords[0], haltestellen.loc[indexH, 'geometry'].coords[0])\n",
    "    indexG = garage.sindex.nearest(geometry=accident_point)[1][0]\n",
    "    accidents_raw.loc[indexU, 'garage_id'] = indexG\n",
    "    accidents_raw.loc[indexU, 'distance_garage'] = math.dist(accident_point.coords[0], haltestellen.loc[indexG, 'geometry'].coords[0])    \n",
    "    \n",
    "haltestellen.drop(columns=['geometry'], inplace=True)\n",
    "garage.drop(columns=['geometry'], inplace=True)\n",
    "accidents_raw = pd.merge(accidents_raw, haltestellen, how='left', left_on='PTS_id', right_index=True)\n",
    "accidents_raw = pd.merge(accidents_raw, garage, how='left', left_on='garage_id', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_raw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "accidents_raw['geometry'].plot(ax=ax, alpha=0.2, markersize=4)\n",
    "ax.scatter(traffic_coords['EKoord'], traffic_coords['NKoord'], s=50, color='#4d004d', alpha=0.8, marker='D')\n",
    "ax.scatter(weather_coords['E'], weather_coords['N'], s=150, alpha=0.8, color='green', marker='p')\n",
    "ax.scatter(haltestellen_coords['E'], haltestellen_coords['N'], alpha=0.5, s=60, color='orange', marker='v')\n",
    "garage_coords['geometry'].plot(ax=ax, alpha=0.5, markersize=100, color='grey', marker='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Feature Engineering: Traffic Zones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains the geometries and type of all traffic zones in Zurich City."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zonen = gpd.read_file('Datasets/Verkehrszonen.gpkg', geometry='geometry')[['zonentyp_technical', 'umgesetzt_datum', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zonen.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below checks for invalid geometries and finds one at index 126. This issue is then resolved by using the shapely buffer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(307):\n",
    "    if shp.is_valid_reason(zonen['geometry'][i]) != 'Valid Geometry':\n",
    "        print(i, shp.is_valid_reason(zonen['geometry'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zonen['geometry'][126] = shp.buffer(zonen['geometry'][126], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_raw['DateTime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each zone is iterated and checked for which accidents happened inside it and wether it was already instated at the time of the accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, polygon in enumerate(tqdm(zonen['geometry'])):\n",
    "    # Erster Schritt: Verwenden von 'contains' für räumliche Überprüfung\n",
    "    spatial_mask = shp.contains(polygon, accidents_raw['geometry'])\n",
    "\n",
    "    # Zweiter Schritt: Überprüfen des Datums, bei Nan-Werten wird die Bedingung automatisch erfüllt\n",
    "    date_condition = (accidents_raw.loc[spatial_mask, 'DateTime'] >= zonen['umgesetzt_datum'][index]) | pd.isna(zonen['umgesetzt_datum'][index])\n",
    "    \n",
    "    # Kombinieren Sie räumliche und zeitliche Bedingungen\n",
    "    mask = spatial_mask & date_condition\n",
    "    #mask = shp.contains(polygon, accidents_raw['geometry']) and accidents_raw['date'] >= zonen['umgesetzt_datum'][zonen['geometry'] == polygon].values[0]\n",
    "    accidents_raw.loc[mask, 'zone'] = zonen['zonentyp_technical'][zonen['geometry'] == polygon].values[0]\n",
    "    accidents_raw.loc[mask, 'zonenid'] = zonen[zonen['geometry'] == polygon].index[0]\n",
    "    \n",
    "accidents_raw['zone'] = accidents_raw['zone'].fillna('no_zone')\n",
    "accidents_raw['zonenid'] = accidents_raw['zonenid'].fillna('no_zoneid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zonencolors = {'T0': '#802b00', 'T20': '#b36b00', 'T30': '#ffcc00'}\n",
    "zonen['color'] = zonen['zonentyp_technical'].map(zonencolors)\n",
    "zonen_coords = zonen[['geometry', 'color']].copy()\n",
    "zonen.drop(['color'], axis=1, inplace=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "zonen_coords[\"geometry\"].plot(facecolor=zonen_coords['color'], ax=ax, alpha=0.5)\n",
    "accidents_raw['geometry'].plot(ax=ax, alpha=0.2, markersize=4)\n",
    "ax.scatter(traffic_coords['EKoord'], traffic_coords['NKoord'], s=50, color='#4d004d', alpha=0.8, marker='D')\n",
    "ax.scatter(weather_coords['E'], weather_coords['N'], s=150, alpha=0.8, color='green', marker='p')\n",
    "garage_coords['geometry'].plot(ax=ax, alpha=0.5, markersize=100, color='grey', marker='s')\n",
    "ax.scatter(haltestellen_coords['E'], haltestellen_coords['N'], alpha=0.5, s=60, color='orange', marker='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Feature Engineering: Street Network with Speedlimit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset allows us to assign a street and its speedlimit to each accident, as well as calculate the distance to the next intersection and pedestrian crossings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strassen = pd.read_csv('Datasets/Strassen_mit_Geschwindigkeit.csv')[['objectid', 'temporegime_technical', 'umgesetzt_datum', 'geometry']]\n",
    "strassen['geometry'] = strassen['geometry'].apply(shp.wkt.loads)\n",
    "strassen = gpd.GeoDataFrame(strassen, geometry='geometry')\n",
    "strassen['umgesetzt_datum'] = pd.to_datetime(strassen['umgesetzt_datum'], format='%Y%m%d%H%M%S%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strassen.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strassen.loc[~strassen.umgesetzt_datum.isna(), 'temporegime_technical'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset also contains some invalid geometries, that need to be resolved first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(strassen)):\n",
    "    if shp.is_valid_reason(strassen['geometry'][i]) != 'Valid Geometry':\n",
    "        print(i, shp.is_valid_reason(strassen['geometry'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strassen = strassen.drop(2692).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(strassen)):\n",
    "    if strassen['geometry'][i].is_empty:\n",
    "        print(i, strassen['geometry'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(strassen)):\n",
    "    if strassen['geometry'][i].geom_type != 'LineString':\n",
    "        print(i, strassen['geometry'][i].geom_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strassen = strassen.explode(index_parts=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8.1 Streets: Speedlimit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the following graph illustrates, spacial indexing allows us to quickly find the closest street geometry for each accident point and assign the closest one (orange line in the picture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.scatter(accidents_raw['geometry'][1000].x, accidents_raw['geometry'][1000].y, color='green', s=20)\n",
    "strassen[\"geometry\"][100:120].plot(ax=ax, color='blue')\n",
    "for i in range(21):\n",
    "    gpd.GeoSeries(shp.shortest_line(accidents_raw['geometry'][1000], strassen['geometry'][i+100])).plot(ax=ax, color='red')\n",
    "actual_nearest_index = strassen.sindex.nearest(geometry=accidents_raw['geometry'][1000])[1][0]\n",
    "line_gdf = gpd.GeoDataFrame(geometry=[strassen['geometry'].iloc[actual_nearest_index]])\n",
    "line_gdf.plot(ax=ax, color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#für jeden accidents_raw\n",
    "for indexU, accidents_rawpunkt in enumerate(tqdm(accidents_raw['geometry'])):\n",
    "    #umsetzungsdatum der Strassen erstmal ignoriert\n",
    "    #Nächste Strassen durch spatial index finden\n",
    "    indexS = strassen.sindex.nearest(geometry=accidents_rawpunkt)[1][0]\n",
    "    accidents_raw.loc[indexU, 'streetid'] = indexS\n",
    "    accidents_raw.loc[indexU, 'speedlimit'] = strassen.loc[indexS, 'temporegime_technical']\n",
    "\n",
    "accidents_raw['speedlimit'] = accidents_raw['speedlimit'].str[1:3].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_raw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorsstreets = {'T0': '#262626', 'T20': '#590d59', 'T30': '#821782', 'T50': '#862d59', 'T60': '#990033', 'T80': '#993333', 'T100': '#8a0f0f', 'T120': '#e63900', 'T50N30': '#862d59', 'T30N0': '#821782'}\n",
    "strassen['color'] = strassen['temporegime_technical'].map(colorsstreets)\n",
    "strassen['color'].fillna('#3e3e5b', inplace=True)\n",
    "strassen_coords = strassen[['geometry', 'color']].copy()\n",
    "strassen.drop(columns=['color'], inplace=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "zonen_coords['geometry'].plot(facecolor=zonen_coords['color'], ax=ax, alpha=0.5)\n",
    "strassen_coords['geometry'].plot(ax=ax, color=strassen_coords['color'], linewidth=1, alpha=0.8)\n",
    "accidents_raw['geometry'].plot(ax=ax, alpha=0.2, markersize=4)\n",
    "ax.scatter(traffic_coords['EKoord'], traffic_coords['NKoord'], s=50, color='#4d004d', alpha=0.8, marker='D')\n",
    "ax.scatter(weather_coords['E'], weather_coords['N'], s=150, alpha=0.8, color='green', marker='p')\n",
    "garage_coords['geometry'].plot(ax=ax, alpha=0.5, markersize=100, color='grey', marker='s')\n",
    "ax.scatter(haltestellen_coords['E'], haltestellen_coords['N'], alpha=0.5, s=60, color='orange', marker='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8.2 Streets: Intersections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we want to illustrate our approach. As the following graph shows, each intersection is represented as a point in the linestring geometries of at least two different streets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = strassen[strassen['objectid']==3593]['geometry']\n",
    "b = strassen['geometry'][5164]\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "a.plot(ax=ax)\n",
    "for i in range(len(b.coords)):\n",
    "    plt.scatter(b.coords[i][0], b.coords[i][1], c='red', s=50)\n",
    "a2 = strassen[strassen['objectid']==4434]['geometry']\n",
    "b2 = strassen['geometry'][5179]\n",
    "a2.plot(ax=ax)\n",
    "for i in range(len(b2.coords)):\n",
    "    plt.scatter(b2.coords[i][0], b2.coords[i][1], c='red', s=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "koordinaten = [(index, coord) for index, geometry in enumerate(strassen['geometry']) for coord in geometry.coords]\n",
    "koordinaten = list(set(koordinaten)) # remove duplicates (same point on same linestring)\n",
    "counter = Counter(coord for index, coord in koordinaten)\n",
    "koordinaten = [entry for entry in koordinaten if counter[entry[1]] > 1]\n",
    "potentielle_kreuzungen = {coord: [index for index, c in koordinaten if c == coord] for coord, count in tqdm(counter.items()) if count > 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we found all points that are present in at least two linestrings, we need to filter those out that are just a continuation of the street, so neither is there a dead end at the point, nor is there an intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weiterleitung = {}\n",
    "kreuzungen = potentielle_kreuzungen.copy()\n",
    "\n",
    "for coord, indices in tqdm(potentielle_kreuzungen.items()):\n",
    "    if len(indices) == 2:\n",
    "        link = True\n",
    "        for i in range(2):\n",
    "            for j in range(len(strassen['geometry'][indices[i]].coords)):\n",
    "                if j > 0 and j < len(strassen['geometry'][indices[i]].coords) -1:\n",
    "                    if strassen['geometry'][indices[i]].coords[j] == coord:\n",
    "                        link = False\n",
    "                        break\n",
    "        \n",
    "        if link:\n",
    "            weiterleitung[coord] = indices\n",
    "            kreuzungen.pop(coord, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(kreuzungen) + len(weiterleitung) == len(potentielle_kreuzungen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code calculates the distance of the accident to the next intersection in both ways of the street network. It does this by first utilizing the shortest_line function and assigning each accident point its closest point on the street network. It then 'travels' the street in both directions by jumping from point to point in the linestrings of the street, adding up the distance travelled. At each point it checks wether it is the dictionary wit all intersections. If it does not find an intersection and reaches one end of the street, it checks wether the last point is in the continuation dictionary and continues the search on the next street, otherwise it found a dead end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_intersection_left(point, linestring, lineindex, streetindex):\n",
    "    d = 0\n",
    "    i = -1\n",
    "        \n",
    "    for k in range(lineindex-1, -1, -1): # eine richtung \n",
    "        d += math.dist(linestring.coords[k], linestring.coords[k+1])\n",
    "        \n",
    "        if linestring.coords[k] in kreuzungen:\n",
    "            i = list(kreuzungen.keys()).index(linestring.coords[k])\n",
    "            break\n",
    "        \n",
    "    if i == -1: # no intersection found\n",
    "        point = linestring.coords[0]\n",
    "        if point in weiterleitung:\n",
    "            \n",
    "            if weiterleitung[point][0] != streetindex: # taking the other street at the intersection, not the one we came from\n",
    "                streetindex2 = weiterleitung[point][0]\n",
    "                \n",
    "            elif weiterleitung[point][0] == streetindex:\n",
    "                streetindex2 = weiterleitung[point][1]\n",
    "                \n",
    "            if point == strassen['geometry'][streetindex2].coords[0]: # if the link point is at the beginning of the other linestring\n",
    "                d2, i = find_intersection_right(point, strassen['geometry'][streetindex2], 0, streetindex2)\n",
    "                d += d2\n",
    "                \n",
    "            elif point == strassen['geometry'][streetindex2].coords[len(strassen['geometry'][streetindex2].coords)-1]: # if the link point is at the end of the other linestring\n",
    "                d2, i = find_intersection_left(point, strassen['geometry'][streetindex2], len(strassen['geometry'][streetindex2].coords)-1, streetindex2)\n",
    "                d += d2\n",
    "                    \n",
    "        # else: dead end, take distance to last point, do nothing, i = -1 means dead end\n",
    "    \n",
    "    return d, i\n",
    "        \n",
    "def find_intersection_right(point, linestring, lineindex, streetindex):\n",
    "    d = 0\n",
    "    i = -1\n",
    "         \n",
    "    for l in range(lineindex + 1, len(linestring.coords), 1): # eine richtung \n",
    "        d += math.dist(linestring.coords[l], linestring.coords[l-1])\n",
    "        \n",
    "        if linestring.coords[l] in kreuzungen:\n",
    "            i = list(kreuzungen.keys()).index(linestring.coords[l])\n",
    "            break\n",
    "        \n",
    "    if i == -1: # no intersection found\n",
    "        point = linestring.coords[len(linestring.coords)-1]\n",
    "        if point in weiterleitung:\n",
    "            \n",
    "            if weiterleitung[point][0] != streetindex: # taking the other street at the intersection, not the one we came from\n",
    "                streetindex2 = weiterleitung[point][0]\n",
    "            else:\n",
    "                streetindex2 = weiterleitung[point][1]\n",
    "                \n",
    "            if point == strassen['geometry'][streetindex2].coords[0]: # if the link point is at the beginning of the other linestring\n",
    "                d, i = find_intersection_right(point, strassen['geometry'][streetindex2], 0, streetindex2)\n",
    "                \n",
    "            elif point == strassen['geometry'][streetindex2].coords[len(strassen['geometry'][streetindex2].coords)-1]: # if the link point is at the end of the other linestring\n",
    "                d, i = find_intersection_left(point, strassen['geometry'][streetindex2], len(strassen['geometry'][streetindex2].coords)-1, streetindex2)\n",
    "                    \n",
    "        # else: dead end, take distance to last point, do nothing, i = -1 means dead end\n",
    "    \n",
    "    return d, i\n",
    "\n",
    "# für jeden row\n",
    "# def kreuzungssuche(row):\n",
    "for i, accident_point in enumerate(tqdm(accidents_raw['geometry'])):\n",
    "    d1 = 0\n",
    "    d2 = 0\n",
    "    ds = 0\n",
    "    i1 = -1\n",
    "    i2 = -1\n",
    "    \n",
    "    streetindex = accidents_raw['streetid'][i]\n",
    "    linestring = strassen['geometry'][streetindex]\n",
    "    point_on_linestring = shp.shortest_line(accident_point, linestring).coords[1]\n",
    "\n",
    "    for j in range(len(linestring.coords)-1): # iterate over linestring to find position of point_on_linestring\n",
    "        if point_on_linestring == linestring.coords[j]: # Point already in linestring\n",
    "            if linestring.coords[j] in kreuzungen: \n",
    "                i1 = list(kreuzungen.keys()).index(linestring.coords[j])\n",
    "                i2 = i1\n",
    "                break\n",
    "            else: # Point not in kreuzungen\n",
    "                d1, i1 = find_intersection_left(point_on_linestring, linestring, j, streetindex)\n",
    "                d2, i2 = find_intersection_right(point_on_linestring, linestring, j, streetindex)\n",
    "                break\n",
    "            \n",
    "        elif shp.Point(point_on_linestring).within(shp.LineString([linestring.coords[j], linestring.coords[j+1]]).buffer(0.0000001)):  # point between two points of linestring\n",
    "            new_linestring = shp.LineString([point for point in linestring.coords[:j+1] + [point_on_linestring] + linestring.coords[j+1:]])\n",
    "            d1, i1 = find_intersection_left(point_on_linestring, new_linestring, j+1, streetindex)\n",
    "            d2, i2 = find_intersection_right(point_on_linestring, new_linestring, j+1, streetindex)\n",
    "            break\n",
    "    \n",
    "    ds = math.dist(accident_point.coords[0], point_on_linestring)    \n",
    "    d1 += ds\n",
    "    d2 += ds\n",
    "        \n",
    "    accidents_raw.loc[i, 'distance_intersection_1'] = d1\n",
    "    accidents_raw.loc[i, 'id_intersection_1'] = i1\n",
    "    accidents_raw.loc[i, 'distance_intersection_2'] = d2\n",
    "    accidents_raw.loc[i, 'id_intersection_2'] = i2\n",
    "    accidents_raw.loc[i, 'distance_to_street'] = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_raw['closest_intersection_id'] = accidents_raw[['id_intersection_1', 'id_intersection_2']].min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_raw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "xmin, xmax = 2680500, 2685500\n",
    "ymin, ymax = 1245600, 1250600\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "zonen_coords['geometry'].plot(facecolor=zonen_coords['color'], ax=ax, alpha=0.5)\n",
    "strassen_coords['geometry'].plot(ax=ax, color=strassen_coords['color'], linewidth=1, alpha=0.8)\n",
    "accidents_raw['geometry'].plot(ax=ax, alpha=0.2, markersize=4)\n",
    "ax.scatter(traffic_coords['EKoord'], traffic_coords['NKoord'], s=50, color='#4d004d', alpha=0.8, marker='D')\n",
    "ax.scatter(weather_coords['E'], weather_coords['N'], s=150, alpha=0.8, color='green', marker='p')\n",
    "ax.scatter(*zip(*weiterleitung.keys()), c='#00cc44', s=30, alpha=0.6)\n",
    "ax.scatter(*zip(*kreuzungen.keys()), c='#0033cc', s=30, alpha=0.6, marker='x')\n",
    "garage_coords['geometry'].plot(ax=ax, alpha=0.5, markersize=100, color='grey', marker='s')\n",
    "ax.scatter(haltestellen_coords['E'], haltestellen_coords['N'], alpha=0.5, s=60, color='orange', marker='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Feature Engineering: Pedestrian and Bicycle Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset helps us find pedestrian crossings and types of bicycle paths that might be helpful to our model for predicting personal injuries that are probably especially likely for accidents involving pedestrians or bicycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fvnetz = gpd.read_file('Datasets/Fuss-_und_Velonetz.gpkg', geometry='geometry')[['velo', 'velostreifen', 'veloweg', 'fuss', 'geometry']]\n",
    "fvnetz.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fvnetz['velostreifen'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fvnetz['velostreifen'] = fvnetz['velostreifen'].map(lambda x: 1 if x in ['BOTH', 'FT', 'TF', '1'] else 0) # we disregard one-way paths and just count them as true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fvnetz)):\n",
    "    if fvnetz['geometry'][i].is_empty:\n",
    "        print(i, fvnetz['geometry'][i].geom_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fvnetz)):\n",
    "    if fvnetz['geometry'][i].geom_type != 'LineString':\n",
    "        print(i, fvnetz['geometry'][i].geom_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.9.1 Finding Pedestrian Crossings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code searches for all pedestrain crossings, by checking wether their geometries cross."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines1 = fvnetz.loc[fvnetz.fuss == 1, 'geometry']\n",
    "lines2 = strassen['geometry'].sindex\n",
    "crossings = {}\n",
    "\n",
    "for line1 in tqdm(lines1):\n",
    "    potential_crossings = list(lines2.intersection(line1.bounds))\n",
    "    \n",
    "    for i in potential_crossings:\n",
    "        line2 = strassen.iloc[i]['geometry']\n",
    "        if strassen.iloc[i]['temporegime_technical'] == 'T0':\n",
    "            continue # This is used to filter out all crossings on streets were driving is prohibited, because they themselves are mostly paths for pedestrian use\n",
    "        # Check if the LineStrings cross\n",
    "        if line1.crosses(line2):\n",
    "            \n",
    "            # Find the crossing points and append to the list\n",
    "            intersection = line1.intersection(line2)       \n",
    "            if intersection.geom_type == 'Point':\n",
    "                #rounded_point = (intersection.x.round(2), intersection.y.round(2))  \n",
    "                crossings[intersection] = i\n",
    "            elif intersection.geom_type == 'MultiPoint':\n",
    "                for p in gpd.GeoSeries(intersection).explode(index_parts=True):\n",
    "                    #rounded_point = (p.x.round(2), p.y.round(2))  \n",
    "                    crossings[p] = i\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking wether we calculated the crossings correctly, by plotting exemplary geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [x for x in strassen.loc[4618, 'geometry'].xy[0]]\n",
    "y = [y for y in strassen.loc[4618, 'geometry'].xy[1]]\n",
    "plt.plot(x, y, c='blue')\n",
    "plt.plot(2683457.176, 1248857.039, c='red', marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "velo = fvnetz.loc[fvnetz.velo == 1].reset_index(drop=True)\n",
    "accidents_raw['is_crossing_on_street'] = False\n",
    "accidents_raw['is_crossing_the_same'] = False\n",
    "crossing_keys = list(crossings.keys())\n",
    "crossing_values = list(crossings.values())\n",
    "geo_crossing_keys = gpd.GeoSeries(crossing_keys)\n",
    "\n",
    "for i, accident_point in enumerate(tqdm(accidents_raw['geometry'])):\n",
    "    #Nächste Strassen durch spatial index finden\n",
    "    sindex = velo.sindex.nearest(geometry=accident_point)[1][0]\n",
    "    accidents_raw.loc[i, 'velo_id'] = sindex\n",
    "    accidents_raw.loc[i, 'velo_distance'] = shp.distance(accident_point, velo.loc[sindex, 'geometry'])\n",
    "    accidents_raw.loc[i, 'veloweg'] = velo.loc[sindex, 'veloweg']\n",
    "    accidents_raw.loc[i, 'velostreifen'] = velo.loc[sindex, 'velostreifen']\n",
    "\n",
    "    crossingindex = geo_crossing_keys.sindex.nearest(geometry=accident_point)[1][0]\n",
    "    crossing = crossing_keys[crossingindex]\n",
    "    onstreet = accidents_raw['streetid'][i] in crossing_values\n",
    "    thesame = crossing_values[crossingindex] == accidents_raw['streetid'][i]\n",
    "    \n",
    "    accidents_raw.loc[i, 'distance_to_closest_crossing'] = shp.distance(accident_point, crossing)\n",
    "    accidents_raw.loc[i, 'closest_crossing_id'] = crossingindex\n",
    "    accidents_raw.loc[i, 'is_crossing_on_street'] = onstreet\n",
    "    accidents_raw.loc[i, 'is_crossing_the_same'] = thesame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_raw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "xmin, xmax = 2680500, 2685500\n",
    "ymin, ymax = 1245600, 1250600\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "zonen_coords['geometry'].plot(facecolor=zonen_coords['color'], ax=ax, alpha=0.5)\n",
    "strassen_coords['geometry'].plot(ax=ax, color=strassen_coords['color'], linewidth=1, alpha=0.8)\n",
    "accidents_raw['geometry'].plot(ax=ax, alpha=0.2, markersize=4)\n",
    "ax.scatter(traffic_coords['EKoord'], traffic_coords['NKoord'], s=50, color='#4d004d', alpha=0.8, marker='D')\n",
    "ax.scatter(weather_coords['E'], weather_coords['N'], s=150, alpha=0.8, color='green', marker='p')\n",
    "ax.scatter(*zip(*weiterleitung.keys()), c='#00cc44', s=30, alpha=0.6)\n",
    "ax.scatter(*zip(*kreuzungen.keys()), c='#0033cc', s=30, alpha=0.6, marker='x')\n",
    "fvnetz.loc[fvnetz.fuss == 1, 'geometry'].plot(ax=ax, color='#996633', linewidth=0.3)\n",
    "fvnetz.loc[fvnetz.velostreifen == 1, 'geometry'].plot(ax=ax, color='#00e6b8', linewidth=0.5)\n",
    "fvnetz.loc[fvnetz.veloweg == 1, 'geometry'].plot(ax=ax, color='#004d00', linewidth=0.5)\n",
    "ax.scatter([point.x for point in crossings.keys()], [point.y for point in crossings.keys()], c='#800000', s=20, alpha=0.4)\n",
    "garage_coords['geometry'].plot(ax=ax, alpha=0.5, markersize=100, color='grey', marker='s')\n",
    "ax.scatter(haltestellen_coords['E'], haltestellen_coords['N'], alpha=0.5, s=60, color='orange', marker='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10 Holidays in Zurich City"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finaly we ad a boolean column for the holidays in Zurich City. This includes publicly ackknowledged holidays on the federal, cantonal and local level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = []\n",
    "sechseläutendic = {\n",
    "    2012: date(2012, 4, 16),\n",
    "    2013: date(2013, 4, 15),\n",
    "    2014: date(2014, 4, 28),\n",
    "    2015: date(2015, 4, 13),\n",
    "    2016: date(2016, 4, 18),\n",
    "    2017: date(2017, 4, 24),\n",
    "    2018: date(2018, 4, 16),\n",
    "    2019: date(2019, 4, 8),\n",
    "    2020: date(2020, 4, 20),\n",
    "    2021: date(2021, 4, 19),\n",
    "    2022: date(2022, 4, 25),\n",
    "}\n",
    "\n",
    "knabenschiessendic = {\n",
    "    2012: date(2012, 9, 10),\n",
    "    2013: date(2013, 9, 9),\n",
    "    2014: date(2014, 9, 15),\n",
    "    2015: date(2015, 9, 14),\n",
    "    2016: date(2016, 9, 12),\n",
    "    2017: date(2017, 9, 11),\n",
    "    2018: date(2018, 9, 10),\n",
    "    2019: date(2019, 9, 9),\n",
    "    2020: None,\n",
    "    2021: date(2021, 9, 13),\n",
    "    2022: date(2022, 9, 12),\n",
    "}\n",
    "\n",
    "for year in range(2012, 2023):\n",
    "    vortag_karfreitag = easter(year) - timedelta(days=3)\n",
    "    karfreitag = easter(year) - timedelta(days=2)\n",
    "    easterday = easter(year)\n",
    "    ostermontag = easter(year) + timedelta(days=1)\n",
    "    vortag_auffahrt = easter(year) + timedelta(days=38)\n",
    "    auffahrt = easter(year) + timedelta(days=39)  # Ascension Day is 39 days after Easter\n",
    "    pfingstmontag = easter(year) + timedelta(days=49)  # Whit Monday is 49 days after Easter\n",
    "    sechseläuten = sechseläutendic[year]\n",
    "    knabenschiessen = knabenschiessendic[year]\n",
    "    \n",
    "    holidays.extend([\n",
    "        vortag_karfreitag,\n",
    "        karfreitag,\n",
    "        easterday,\n",
    "        ostermontag,\n",
    "        vortag_auffahrt,\n",
    "        auffahrt,\n",
    "        pfingstmontag,\n",
    "        sechseläuten,\n",
    "        knabenschiessen,\n",
    "        date(year, 2, 1),  # Berchtolds Day\n",
    "        date(year, 1, 1),  # New Year's Day\n",
    "        date(year, 1, 2),  # Second Day of New Year\n",
    "        date(year, 5, 1),  # Labor Day\n",
    "        date(year, 8, 1),  # Swiss National Day\n",
    "        date(year, 12, 25),  # Christmas Day\n",
    "        date(year, 12, 26),  # St. Stephen's Day\n",
    "    ])\n",
    "\n",
    "accidents_raw['is_holiday'] = accidents_raw['DateTime'].dt.date.isin(holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parks = gpd.read_file('Datasets/Parks.gpkg')[[('geometry')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.11 Final Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing our label. as1 to 3 means personal injury, as0 means only property damage\n",
    "accidents_raw['Severity'] = accidents_raw['AccidentSeverityCategory'].replace({'as1':1, 'as2':1, 'as3': 1,'as4':0,}).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the final raw dataset\n",
    "accidents_raw.to_csv('accidents_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw dataset can be loaded back in at any time, without going through the preprocessing again\n",
    "accidents_raw = pd.read_csv('accidents_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents = pd.DataFrame(accidents_raw.drop(columns=['DateTime', 'Date', 'geometry', 'AccidentUID', 'AccidentLocation_CHLV95_E', 'AccidentLocation_CHLV95_N', 'Date', 'AccidentYear',\n",
    "                                                      'velo_id', 'id_intersection_1', 'id_intersection_2', 'closest_crossing_id', 'streetid', 'closest_intersection_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents = pd.get_dummies(accidents, columns=['AccidentType', 'RoadType', 'zone', 'AccidentMonth', 'AccidentWeekDay', 'AccidentHour', 'CountingStation_MinDistance', 'CountingStation_2thMinDistance', 'CountingStation_3thMinDistance', 'PTS_id', 'garage_id', 'zonenid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents = accidents.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"accidents_raw.csv\")\n",
    "df = df.dropna(subset=['CountingStation_MinDistance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Analysis by time of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_per_hour = df.groupby(['AccidentHour', 'Severity']).size().unstack(fill_value=0).reset_index()\n",
    "accidents_per_hour[\"sum\"] = accidents_per_hour[0] + accidents_per_hour[1] # Sum of all accident types (0 is equal to 'Accident with property damage' and 1 is equal to 'Accident with personal injury')\n",
    "accidents_per_hour[\"Accidents with property damage %\"] = accidents_per_hour[0] / accidents_per_hour[\"sum\"] * 100\n",
    "accidents_per_hour[\"Accidents with personal injury %\"] = accidents_per_hour[1] / accidents_per_hour[\"sum\"] * 100\n",
    "accidents_per_hour.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 20))\n",
    "ax = accidents_per_hour.plot(\n",
    "    x=\"AccidentHour\",\n",
    "    y=[\"Accidents with personal injury %\", \"Accidents with property damage %\"],\n",
    "    kind=\"bar\",\n",
    "    color=['#AA4A44', '#20B2AA'],\n",
    "    edgecolor='black',\n",
    "    width=0.8\n",
    ")\n",
    "\n",
    "# To add space for the legend, we can use bbox_to_anchor to position the legend outside of the plot\n",
    "ax.legend(title=\"Accient Category\", bbox_to_anchor=(1, 1), loc='upper left')\n",
    "ax.set_xlabel(\"Uhrzeit\")\n",
    "ax.set_ylabel(\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Analysis by weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_by_day = df.groupby(['AccidentWeekDay', 'Severity']).size().unstack(fill_value=0).reset_index()\n",
    "accidents_by_day[\"sum\"] = accidents_by_day[0] + accidents_by_day[1] # Sum of all accident types (0 is equal to 'Accident with property damage' and 1 is equal to 'Accident with personal injury')\n",
    "accidents_by_day[\"Accidents with property damage %\"] = accidents_by_day[0] / accidents_by_day[\"sum\"] * 100\n",
    "accidents_by_day[\"Accidents with personal injury %\"] = accidents_by_day[1] / accidents_by_day[\"sum\"] * 100\n",
    "\n",
    "day_mapping = {\n",
    "    1:\"Monday\",\n",
    "    2:\"Tuesday\",\n",
    "    3:\"Wednesday\",\n",
    "    4:\"Thursday\",\n",
    "    5:\"Friday\",\n",
    "    6:\"Saturday\",\n",
    "    7:\"Sunday\"\n",
    "}\n",
    "accidents_by_day['AccidentWeekDay_en'] = accidents_by_day['AccidentWeekDay'].map(day_mapping)\n",
    "accidents_by_day.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 20))\n",
    "ax = accidents_by_day.plot(\n",
    "    x=\"AccidentWeekDay_en\",\n",
    "    y=[\"Accidents with personal injury %\", \"Accidents with property damage %\"],\n",
    "    kind=\"bar\",\n",
    "    color=['#AA4A44', '#20B2AA'],\n",
    "    edgecolor='black',\n",
    "    width=0.8\n",
    ")\n",
    "\n",
    "# To add space for the legend, we can use bbox_to_anchor to position the legend outside of the plot\n",
    "ax.legend(title=\"Accident Category\", bbox_to_anchor=(1, 1), loc='upper left')\n",
    "ax.set_xlabel(\"Weekday\")\n",
    "ax.set_ylabel(\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Analysis by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_by_month = df.groupby(['AccidentMonth', 'Severity']).size().unstack(fill_value=0).reset_index()\n",
    "\n",
    "accidents_by_month[\"sum\"] = accidents_by_month[0] + accidents_by_month[1]\n",
    "accidents_by_month[\"Accidents with property damage %\"] = accidents_by_month[0] / accidents_by_month[\"sum\"] * 100\n",
    "accidents_by_month[\"Accidents with personal injury %\"] = accidents_by_month[1] / accidents_by_month[\"sum\"] * 100\n",
    "\n",
    "\n",
    "month_mapping = {\n",
    "    1:\"January\",\n",
    "    2:\"February\",\n",
    "    3:\"March\",\n",
    "    4:\"April\",\n",
    "    5:\"May\",\n",
    "    6:\"June\",\n",
    "    7:\"July\",\n",
    "    8:\"August\",\n",
    "    9:\"September\",\n",
    "    10:\"October\",\n",
    "    11:\"November\",\n",
    "    12:\"December\"\n",
    "}\n",
    "\n",
    "accidents_by_month['AccidentMonth_en'] = accidents_by_month['AccidentMonth'].map(month_mapping)\n",
    "accidents_by_month.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 20))\n",
    "ax = accidents_by_month.plot(\n",
    "    x=\"AccidentMonth_en\",\n",
    "    y=[\"Accidents with personal injury %\", \"Accidents with property damage %\"],\n",
    "    kind=\"bar\",\n",
    "    color=['#AA4A44', '#20B2AA'],\n",
    "    edgecolor='black',\n",
    "    width=0.8\n",
    ")\n",
    "\n",
    "# To add space for the legend, we can use bbox_to_anchor to position the legend outside of the plot\n",
    "ax.legend(title=\"Accident Category\", bbox_to_anchor=(1, 1), loc='upper left')\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel(\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Analysis by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_by_year = df.groupby(['AccidentYear', 'Severity']).size().unstack(fill_value=0).reset_index()\n",
    "\n",
    "accidents_by_year[\"sum\"] = accidents_by_year[0] + accidents_by_year[1]\n",
    "accidents_by_year[\"Accidents with property damage %\"] = accidents_by_year[0] / accidents_by_year[\"sum\"] * 100\n",
    "accidents_by_year[\"Accidents with personal injury %\"] = accidents_by_year[1] / accidents_by_year[\"sum\"] * 100\n",
    "accidents_by_year.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 20))\n",
    "ax = accidents_by_year.plot(\n",
    "    x=\"AccidentYear\",\n",
    "    y=[\"Accidents with personal injury %\", \"Accidents with property damage %\"],\n",
    "    kind=\"bar\",\n",
    "    color=['#AA4A44', '#20B2AA'],\n",
    "    edgecolor='black',\n",
    "    width=0.8\n",
    ")\n",
    "\n",
    "# To add space for the legend, we can use bbox_to_anchor to position the legend outside of the plot\n",
    "ax.legend(title=\"Accident Category\", bbox_to_anchor=(1, 1), loc='upper left')\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Analysis by roadtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_by_roadtype = df.groupby(['RoadType', 'Severity']).size().unstack(fill_value=0).reset_index()\n",
    "\n",
    "accidents_by_roadtype[\"sum\"] = accidents_by_roadtype[0] + accidents_by_roadtype[1]\n",
    "accidents_by_roadtype[\"Accidents with property damage %\"] = accidents_by_roadtype[0] / accidents_by_roadtype[\"sum\"] * 100\n",
    "accidents_by_roadtype[\"Accidents with personal injury %\"] = accidents_by_roadtype[1] / accidents_by_roadtype[\"sum\"] * 100\n",
    "\n",
    "\n",
    "roadtype_mapping = {\n",
    "    \"rt432\":\"Principal road\",\n",
    "    \"rt433\":\"Minor road\",\n",
    "    \"rt439\":\"Other\",\n",
    "}\n",
    "\n",
    "accidents_by_roadtype['RoadType_en'] = accidents_by_roadtype['RoadType'].map(roadtype_mapping)\n",
    "accidents_by_roadtype.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 20))\n",
    "ax = accidents_by_roadtype.plot(\n",
    "    x=\"RoadType_en\",\n",
    "    y=[\"Accidents with personal injury %\", \"Accidents with property damage %\"],\n",
    "    kind=\"bar\",\n",
    "    color=['#AA4A44', '#20B2AA'],\n",
    "    edgecolor='black',\n",
    "    width=0.8\n",
    ")\n",
    "\n",
    "# To add space for the legend, we can use bbox_to_anchor to position the legend outside of the plot\n",
    "ax.legend(title=\"Accident Category\", bbox_to_anchor=(1, 1), loc='upper left')\n",
    "ax.set_xlabel(\"Road Type\")\n",
    "ax.set_ylabel(\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Analysis by Accident Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_by_accidenttype = df.groupby(['AccidentType', 'Severity']).size().unstack(fill_value=0).reset_index()\n",
    "accidents_by_accidenttype[\"sum\"] = accidents_by_accidenttype[0] + accidents_by_accidenttype[1]\n",
    "accidents_by_accidenttype[\"Accidents with property damage %\"] = accidents_by_accidenttype[0] / accidents_by_accidenttype[\"sum\"] * 100\n",
    "accidents_by_accidenttype[\"Accidents with personal injury %\"] = accidents_by_accidenttype[1] / accidents_by_accidenttype[\"sum\"] * 100\n",
    "\n",
    "accidenttype_mapping = {\n",
    "    \"at0\":\"Accident with skidding or self-accident\",\n",
    "    \"at00\":\"Other\",\n",
    "    \"at1\":\"Accident when overtaking or changing lanes\",\n",
    "    \"at2\":\"Accident with rear-end collision\",\n",
    "    \"at3\":\"Accident when turning left or right\",\n",
    "    \"at4\":\"Accident when turning-into main road\",\n",
    "    \"at5\":\"Accident when crossing the lane(s)\",\n",
    "    \"at6\":\"Accident with head-on collision\",\n",
    "    \"at7\":\"Accident when parking\",\n",
    "    \"at8\":\"Accident involving pedestrian(s)\",\n",
    "    \"at9\":\"Accident involving animal(s)\"\n",
    "}\n",
    "\n",
    "accidents_by_accidenttype['AccidentType_en'] = accidents_by_accidenttype['AccidentType'].map(accidenttype_mapping)\n",
    "accidents_by_accidenttype.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 20))\n",
    "ax = accidents_by_accidenttype.plot(\n",
    "    x=\"AccidentType_en\",\n",
    "    y=[\"Accidents with personal injury %\", \"Accidents with property damage %\"],\n",
    "    kind=\"bar\",\n",
    "    color=['#AA4A44', '#20B2AA'],\n",
    "    edgecolor='black',\n",
    "    width=0.8\n",
    ")\n",
    "\n",
    "# To add space for the legend, we can use bbox_to_anchor to position the legend outside of the plot\n",
    "ax.legend(title=\"Accident Category\", bbox_to_anchor=(1, 1), loc='upper left')\n",
    "ax.set_xlabel(\"Accident Type\")\n",
    "ax.set_ylabel(\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Analysis of whether motorcycle is involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_by_motorcycle = df.groupby(['AccidentInvolvingMotorcycle', 'Severity']).size().unstack(fill_value=0).reset_index()\n",
    "accidents_by_motorcycle[\"sum\"] = accidents_by_motorcycle[0] + accidents_by_motorcycle[1]\n",
    "accidents_by_motorcycle[\"Accidents with property damage %\"] = accidents_by_motorcycle[0] / accidents_by_motorcycle[\"sum\"] * 100\n",
    "accidents_by_motorcycle[\"Accidents with personal injury %\"] = accidents_by_motorcycle[1] / accidents_by_motorcycle[\"sum\"] * 100\n",
    "accidents_by_motorcycle.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 20))\n",
    "ax = accidents_by_motorcycle.plot(\n",
    "    x=\"AccidentInvolvingMotorcycle\",\n",
    "    y=[\"Accidents with personal injury %\", \"Accidents with property damage %\"],\n",
    "    kind=\"bar\",\n",
    "    color=['#AA4A44', '#20B2AA'],\n",
    "    edgecolor='black',\n",
    "    width=0.8\n",
    ")\n",
    "\n",
    "# To add space for the legend, we can use bbox_to_anchor to position the legend outside of the plot\n",
    "ax.legend(title=\"Accident Category\", bbox_to_anchor=(1, 1), loc='upper left')\n",
    "ax.set_ylabel(\"%\")\n",
    "ax.set_xlabel(\"Motorcycle involved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Analysis by Next Traffic Frequency Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traffic = df.query(\"CountingStation_MinDistance in ['Kornhausbrücke Limmatplatz','Stauffacherstrasse (Feldstrasse) Seebahnstrasse','Stauffacherstrasse (Feldstrasse) Seebahnstrasse','Binzmühlestrasse (Thurgauerstrasse) Schwamendingen','Europabrücke (Max Högger- und Würzgrabenstrasse) Altstetten','Seebahnstrasse (Kalkbreitestrasse) Birmensdorferstrasse']\")\n",
    "accidents_by_countingstation_mindistance = df_traffic.groupby(['CountingStation_MinDistance', 'Severity']).size().unstack(fill_value=0).reset_index()\n",
    "accidents_by_countingstation_mindistance[\"sum\"] = accidents_by_countingstation_mindistance[0] + accidents_by_countingstation_mindistance[1]\n",
    "accidents_by_countingstation_mindistance[\"Accidents with property damage %\"] = accidents_by_countingstation_mindistance[0] / accidents_by_countingstation_mindistance[\"sum\"] * 100\n",
    "accidents_by_countingstation_mindistance[\"Accidents with personal injury %\"] = accidents_by_countingstation_mindistance[1] / accidents_by_countingstation_mindistance[\"sum\"] * 100\n",
    "accidents_by_countingstation_mindistance.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 20))\n",
    "ax = accidents_by_countingstation_mindistance.plot(\n",
    "    x=\"CountingStation_MinDistance\",\n",
    "    y=[\"Accidents with personal injury %\", \"Accidents with property damage %\"],\n",
    "    kind=\"bar\",\n",
    "    color=['#AA4A44', '#20B2AA'],\n",
    "    edgecolor='black',\n",
    "    width=0.8\n",
    ")\n",
    "\n",
    "# To add space for the legend, we can use bbox_to_anchor to position the legend outside of the plot\n",
    "ax.legend(title=\"Accident Category\", bbox_to_anchor=(1, 1), loc='upper left')\n",
    "ax.set_ylabel(\"%\")\n",
    "ax.set_xlabel(\"Traffic frequency counting stations\")\n",
    "ax.set_title(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Analysis by Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = df[[\"Severity\",\"RainDur (min)_mean-1h\",\"RainDur (min)_mean-2h\",\n",
    "              \"StrGlo (W/m2)_mean-1h\",\"StrGlo (W/m2)_mean-2h\",\n",
    "              \"Temp (°C)_mean-1h\",\"Temp (°C)_mean-2h\",\n",
    "              \"WD (°)_mean-1h\",\"WD (°)_mean-2h\",\n",
    "              \"WVv (m/s)_mean-1h\",\"WVv (m/s)_mean-2h\",\n",
    "              \"p (hPa)_mean-1h\",\"p (hPa)_mean-2h\",\n",
    "              \"Hr (%Hr)_mean-1h\",\"Hr (%Hr)_mean-2h\"]]\n",
    "weather.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,20))\n",
    "sns.heatmap(weather.corr(), annot=True, cmap=\"YlGnBu\")\n",
    "plt.title(\"Correlation matrix for the weather data set\", fontsize=32, fontweight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Analysis of traffic volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic = df[[\"Severity\",\n",
    "            \"CountingStation_MinDistance\",\"MinDistance\",\"AnzFahrzeuge_-1h\",\"AnzFahrzeuge_-2h\",\n",
    "            \"2thMinDistance\",\"AnzFahrzeuge_2thMinDistance_-1h\",\"AnzFahrzeuge_2thMinDistance_-2h\",\n",
    "            \"3thMinDistance\",\"AnzFahrzeuge_3thMinDistance_-1h\",\"AnzFahrzeuge_3thMinDistance_-2h\",]]\n",
    "trafficfrequency = pd.DataFrame(traffic[\"CountingStation_MinDistance\"].value_counts()).head(10).reset_index()\n",
    "dummies = pd.get_dummies(trafficfrequency[['CountingStation_MinDistance']])\n",
    "traffic = pd.concat([traffic, dummies], axis=1)\n",
    "traffic.drop(columns=['CountingStation_MinDistance'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 20))\n",
    "sns.heatmap(traffic.corr(), annot=True, cmap=\"YlGnBu\")\n",
    "plt.title(\"Correlation matrix for the traffic data set for the 10 measuring stations with the most measurements\", fontsize=24, fontweight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data (replace this with your data)\n",
    "X = accidents.drop(columns=['AccidentSeverityCategory', 'Severity'])\n",
    "y = accidents[['AccidentSeverityCategory', 'Severity']]\n",
    "\n",
    "# Split the data into training, validating and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=72)\n",
    "\n",
    "z_train = y_train['AccidentSeverityCategory']\n",
    "z_val = y_val['AccidentSeverityCategory']\n",
    "z_test = y_test['AccidentSeverityCategory']\n",
    "\n",
    "y_train = y_train['Severity']\n",
    "y_val = y_val['Severity']\n",
    "y_test = y_test['Severity']\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Anwenden von SMOTE auf den Trainingsdatensatz\n",
    "smote = SMOTE(random_state=41)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def plot_results(y_probs, y_true=y_val):    \n",
    "    \n",
    "    desired_recall = 0.9 \n",
    "    thresholds = np.arange(-0.2, 1, 0.005) \n",
    "    best_threshold = 0 \n",
    "    best_roc_score = 0 \n",
    "    recall_scores = [] \n",
    "    roc_scores = [] \n",
    "    \n",
    "    for threshold in thresholds: \n",
    "        y_pred_val_custom_threshold = (y_probs > threshold).astype(int) \n",
    "        current_recall = recall_score(y_true, y_pred_val_custom_threshold) \n",
    "        \n",
    "        if current_recall >= desired_recall: \n",
    "            current_roc_score = roc_auc_score(y_true, y_pred_val_custom_threshold)\n",
    "            recall_scores.append(current_recall) \n",
    "            roc_scores.append(current_roc_score) \n",
    "    \n",
    "    best_threshold_index = np.argmax(roc_scores) \n",
    "    best_threshold_value = thresholds[best_threshold_index] \n",
    "\n",
    "    best_roc_score = roc_scores[best_threshold_index] \n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_probs > best_threshold_value)\n",
    "\n",
    "    # Plot ROC curve and confusion matrix side by side\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 5)) \n",
    "\n",
    "    # Plot ROC curve and Recall scores\n",
    "    ax[0].plot(thresholds[:len(recall_scores)], recall_scores, label='Recall')\n",
    "    ax[0].plot(thresholds[:len(roc_scores)], roc_scores, label='ROC Score')\n",
    "    ax[0].scatter([best_threshold_value], [best_roc_score], color='red', marker='o', label='Best Threshold')\n",
    "    ax[0].set_xlabel('Threshold')\n",
    "    ax[0].set_ylabel('Score')\n",
    "    ax[0].legend()\n",
    "    ax[0].set_title('Recall und ROC Score in Abhängigkeit vom Threshold')\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Positive'])\n",
    "    disp.plot(cmap='Blues', ax=ax[1])\n",
    "    ax[1].set_title('Confusion Matrix at Threshold {:.3f}'.format(best_threshold))\n",
    "\n",
    "    ## Add text with best threshold and ROC score above the plots\n",
    "    text = f'Best Threshold: {best_threshold_value:.3f}, Best ROC Score: {best_roc_score:.3f}, Recall: {recall_score(y_true, y_probs > best_threshold_value):.3f}'\n",
    "    fig.text(0.5, 1.05, text, ha='center', va='center', fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Basic Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# GridSearchCV-Objekt erstellen\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen auf dem Validierungsset machen\n",
    "y_prob_val_rf1 = rf_classifier.predict_proba(X_val)[:, 1]\n",
    "\n",
    "plot_results(y_prob_val_rf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Hyperparamter tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will optimize our model so that the ROC-AUC score is maximized, regardless of recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametergitter für die Anzahl der Bäume\n",
    "param_grid = {\n",
    "    'criterion': [\"gini\",\"entropy\"],\n",
    "    'max_depth': [10,None],\n",
    "    'min_samples_split': [2,5,7],\n",
    "    'min_samples_leaf': [1,2,5]\n",
    "}\n",
    "\n",
    "# Random Forest Classifier erstellen\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# GridSearchCV-Objekt erstellen\n",
    "grid_search = GridSearchCV(rf_classifier, param_grid, cv=3)\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Beste Parameter abrufen\n",
    "best_params = grid_search.best_estimator_.get_params()\n",
    "print(\"Best Parameters: \", best_params)\n",
    "\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "# Vorhersagen auf dem Validierungsset machen\n",
    "y_prob_val_rf1 = best_estimator.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Benutzerdefinierten Recall-Wert festlegen\n",
    "desired_recall = 0.3  \n",
    "\n",
    "# Finden Sie den Schwellenwert für den gewünschten Recall\n",
    "thresholds = np.arange(0, 1.05, 0.05)\n",
    "best_threshold = 0\n",
    "best_roc_score = 0\n",
    "\n",
    "recall_scores = []\n",
    "roc_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_val_rf1_custom_threshold = (y_prob_val_rf1 > threshold).astype(int)\n",
    "    current_recall = recall_score(y_val, y_pred_val_rf1_custom_threshold)\n",
    "    \n",
    "    if current_recall >= desired_recall:\n",
    "        current_roc_score = roc_auc_score(y_val, y_pred_val_rf1_custom_threshold)\n",
    "        \n",
    "        recall_scores.append(current_recall)\n",
    "        roc_scores.append(current_roc_score)\n",
    "\n",
    "\n",
    "best_threshold_index = np.argmax(roc_scores)\n",
    "best_threshold_value = thresholds[best_threshold_index]\n",
    "best_roc_score = roc_scores[best_threshold_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob_val_rf1 = best_estimator.predict_proba(X_val)[:, 1]\n",
    "y_pred_val_rf1 = (y_prob_val_rf1 > best_threshold_value).astype(int)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# First plot\n",
    "axs[0].plot(thresholds[:len(recall_scores)], recall_scores, label='Recall')\n",
    "axs[0].plot(thresholds[:len(roc_scores)], roc_scores, label='ROC Score')\n",
    "axs[0].scatter([best_threshold_value], [best_roc_score], color='red', marker='o', label='Best Threshold')\n",
    "axs[0].set_xlabel('Threshold')\n",
    "axs[0].set_ylabel('Score')\n",
    "axs[0].legend()\n",
    "axs[0].set_title('Recall und ROC Score (validation set)')\n",
    "axs[0].text(0.5, -0.15, f\"Bester Threshold: {best_threshold_value:.2f}\\nBester ROC Score: {best_roc_score:.2f}\", \n",
    "            transform=axs[0].transAxes, horizontalalignment='center', verticalalignment='center', fontsize=16)\n",
    "\n",
    "\n",
    "# Second plot\n",
    "cm_val = confusion_matrix(y_val, y_pred_val_rf1)\n",
    "sns.heatmap(cm_val, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'], ax=axs[1])\n",
    "axs[1].set_title('Confusion matrix (validation set)')\n",
    "axs[1].set_xlabel('Predicted Values')\n",
    "axs[1].set_ylabel('Actual Values')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Given a chosen recall, train a model that maximizes the ROC-AUC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametergitter für die Anzahl der Bäume\n",
    "param_grid = {\n",
    "    'criterion': [\"gini\",\"entropy\"],\n",
    "    'max_depth': [10,None],\n",
    "    'min_samples_split': [2,5,7],\n",
    "    'min_samples_leaf': [1,2,5]\n",
    "}\n",
    "\n",
    "# Random Forest Classifier erstellen\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# GridSearchCV-Objekt erstellen\n",
    "grid_search = GridSearchCV(rf_classifier, param_grid, cv=3)\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Beste Parameter abrufen\n",
    "best_params = grid_search.best_estimator_.get_params()\n",
    "print(\"Best Parameters: \", best_params)\n",
    "\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "# Vorhersagen auf dem Validierungsset machen\n",
    "y_prob_val_rf2 = best_estimator.predict_proba(X_val)[:, 1]\n",
    "\n",
    "plot_results(y_prob_val_rf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ As we have seen, there is a trade-off between maximizing precision and recall. We can't maximize both at the same time.\n",
    "+ In our case, we would like to reduce the number of false negative results, as it can have fatal consequences if personal injury (a positive) is classified as property damage (a negative). We accept that this will increase the number of false positives. This meant: It would be better for an ambulance to drive to the scene of an accident in vain than for an injured person not to be offered the necessary outpatient help.\n",
    "+ For this reason we choose a recall of 0.9 for the other models.\n",
    "+ We also accept that a given recall of 0.9 may not give us the maximum possible value for the \"Area Under the Curve\" of the “Receiver Operating Characteristic” curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4 Hyperparamter tuning with RandomSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Although hyperparameter tuning with GridSearchCV is very accurate, this type of hyperparameter tuning requires a lot of computing power. For this reason, in a next step we will use Scikilearn's RandomizedSearchCV function.\n",
    "+ The RandomizedSearchCV-function searches randomly selected combinations of the specified hyperparameter values. It performs a random search and only tries a set number of combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=41)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Parametergitter für die Anzahl der Bäume\n",
    "param_grid = {\n",
    "    'criterion': [\"gini\",\"entropy\"],\n",
    "    'max_depth': [10,20,30,None],\n",
    "    'min_samples_split': [2,5,7,9],\n",
    "    'min_samples_leaf': [2,5,7,9]\n",
    "}\n",
    "\n",
    "# Random Forest Classifier erstellen\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Weitere Optimierung der Hyperparameter\n",
    "grid_search = RandomizedSearchCV(rf_classifier, param_grid, cv=3, n_iter=10, random_state=42)\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Vorhersagen auf dem Validierungsset machen\n",
    "y_prob_val_rf3 = grid_search.best_estimator_.predict_proba(X_val)[:, 1]\n",
    "randomforestcv =  grid_search.best_estimator_\n",
    "\n",
    "# Hier kommt die Funktion von Gabriel\n",
    "plot_results(y_prob_val_rf3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Since the ROC-AUC score, which was calculated once by the model where we selected the hyperparamters with the GridSearchCV and once by the other model where we selected the hyperparamters with the RandomizedSearchCV function, does not differ greatly, we will use further the RandomizedSearchCV function below to find the optimal hyperparameters for the decision tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes-Modell erstellen\n",
    "nb_classifier = GaussianNB()\n",
    "\n",
    "# Modell auf den resamplten Trainingsdatensatz anpassen\n",
    "nb_classifier.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Vorhersagen auf dem Validierungsset machen\n",
    "y_prob_val_nb = nb_classifier.predict_proba(X_val)[:, 1]\n",
    "\n",
    "plot_results(y_prob_val_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the naive Bayes classifier is not much better than a model that would randomly choose between the two classes. Since we have set the recall to 0.9 and the model cannot distinguish well between the two classes, class 1 is predicted significantly more often, which means that the number of false negatives is low and therefore the recall is high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 Untuned Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ First we will use an untuned decision tree as a model and then search for the optimal hyperparameters with RandomizedSearchCV and cross-validation.\n",
    "+ First of all, for example, we will plot the decision tree (but not with max depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train,y_train)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "plot_tree(tree, label=\"root\", filled=True, max_depth=1, \n",
    "          feature_names=X.columns, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Wahrscheinlichkeiten für die positive Klasse (Klasse 1) auf dem Validierungsset erhalten\n",
    "y_prob_val_dt1 = tree.predict_proba(X_val)[:, 1]\n",
    "\n",
    "plot_results(y_prob_val_dt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Hyperprameter Tuning - Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametergitter für die Anzahl der Bäume\n",
    "param_grid = {\n",
    "    'criterion': [\"gini\",\"entropy\"],\n",
    "    'max_depth': [10,20,30,None],\n",
    "    'min_samples_split': [2,5,7,9],\n",
    "    'min_samples_leaf': [2,5,7,9]\n",
    "}\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "grid_search = RandomizedSearchCV(tree, param_grid, cv=3, n_iter=10)\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Beste Parameter abrufen\n",
    "best_estimator = grid_search.best_estimator_\n",
    "y_prob_val_dt2 = best_estimator.predict_proba(X_val)[:, 1]\n",
    "\n",
    "plot_results(y_prob_val_dt2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1 Basic Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the logistic regression model\n",
    "log = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log.fit(X_train, y_train)\n",
    "\n",
    "# Plot ROC curve\n",
    "y_prob_val_LR1 = log.predict_proba(X_val)[:, 1]\n",
    "\n",
    "plot_results(y_prob_val_LR1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2 Hyperparameter Tuning - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define logistic regression models with L1 and L2 regularization\n",
    "lasso_model = LogisticRegression(penalty='l1', solver='saga', max_iter=1000, tol=1e-3, n_jobs=-1)\n",
    "ridge_model = LogisticRegression(penalty='l2', solver='saga', max_iter=1000, tol=1e-3, n_jobs=-1)\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10]}\n",
    "\n",
    "# Define the scoring metric (ROC-AUC) and specify that you want to maximize it\n",
    "scoring = {'AUC': 'roc_auc', 'Recall': make_scorer(recall_score, pos_label=1)}\n",
    "\n",
    "# Perform randomized search with cross-validation\n",
    "lasso_random_search = RandomizedSearchCV(lasso_model, param_distributions=param_grid, n_iter=3, scoring=scoring, cv=3, refit='Recall', n_jobs=-1)\n",
    "ridge_random_search = RandomizedSearchCV(ridge_model, param_distributions=param_grid, n_iter=3, scoring=scoring, cv=3, refit='Recall', n_jobs=-1)\n",
    "\n",
    "# Fit the models with the best parameters\n",
    "lasso_random_search.fit(X_train_resampled, y_train_resampled)\n",
    "ridge_random_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best models\n",
    "best_lasso_model = lasso_random_search.best_estimator_\n",
    "best_ridge_model = ridge_random_search.best_estimator_\n",
    "\n",
    "y_prob_val_lasso = best_lasso_model.predict_proba(X_val)[:, 1]\n",
    "y_prob_val_ridge = best_ridge_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "plot_results(y_prob_val_lasso)\n",
    "plot_results(y_prob_val_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried two methods to oversample our inbalanced dataset. But we figured out that the neuronal network performs slighly worse than without oversampling. Therefore we won't use the oversampling method for neuronal networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 Model 1 (Model for testing out different layers and neuron counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baselinemodel2 with a hidden layer consisting of 500 neurons\n",
    "def create_baseline2():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(500, input_shape=(1523,), activation='relu')) \n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate baseline model with standardized dataset\n",
    "estimator2 = KerasClassifier(model=create_baseline2, epochs=8, batch_size=20, verbose=0, metrics=[recall_score])\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "results = cross_val_score(estimator2, X_train, y_train, cv=kfold)\n",
    "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob_val_nn2 = estimator2.predict_proba(X_val)[:,1]\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(y_prob_val_nn2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that this type of network has espacially a big std. Let's compare this network with another one which has less neurons in the hidden layer and compare both networks. We can observe that the network performs slightly better on onseen data but it seems that it performs better much better in training. The reason for this could be that we slightly overfit the network on the training data. We can conclude that the performance of the second network is as good as the first but computational cost is much lower. Let's see how a neronal network with two hidden layers performs. As we could conclude from this experiment, we can observe that with less neurons in the hidden layer, the network performs better on the training data and slightly better on the test data. Therfore we will try to set the neurons in the second hidden layer to 250 and see how the network performs. The idea here is that the network is given the opportunity to model all input variables before being bottlenecked and forced to halve the representational capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 Hyperparameter Tuning 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline7(hp):\n",
    "    model = Sequential()\n",
    "    for i in range(hp.Int('layers', 2, 6)):\n",
    "        model.add(Dense(\n",
    "            units=hp.Int('units_' + str(i), 20, 300, step=35), input_shape=(1523,),\n",
    "            activation=hp.Choice('act_' + str(i), ['relu', 'sigmoid'])\n",
    "        ))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tuner1 = RandomSearch(\n",
    "    create_baseline7,\n",
    "    objective='accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=6,\n",
    "    directory='my_dir',\n",
    "    project_name='tuner1'\n",
    ")\n",
    "\n",
    "tuner1.search(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner1.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = tuner1.get_best_models(num_models=2) # we will select the best 3 models and want to see how they perform on unseen data\n",
    "y_prob_val_nn3 = best_models[0].predict_proba(X_val)[:,1]\n",
    "y_prob_val_nn4 = best_models[1].predict_proba(X_val)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's call our plot function to plot the ROC of each model compared to the recall\n",
    "plot_results(y_prob_val_nn3)\n",
    "plot_results(y_prob_val_nn4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3 Hyperparameter Tuning 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline8(hp):\n",
    "    model = Sequential()\n",
    "    for i in range(hp.Int('layers', 2, 10)):\n",
    "        model.add(Dense(\n",
    "            units=hp.Int('units_' + str(i), 10, 200, step=20), input_shape=(1523,),\n",
    "            activation=hp.Choice('act_' + str(i), ['relu', 'sigmoid'])\n",
    "        ))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.AUC(curve='ROC')])\n",
    "    return model\n",
    "\n",
    "tuner8 = RandomSearch(\n",
    "    create_baseline8,\n",
    "    objective=Objective('auc', direction='max'),\n",
    "    max_trials=5,\n",
    "    executions_per_trial=5,\n",
    "    directory='my_dir',\n",
    "    project_name='tuner2'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner8.search(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner8.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model8 = tuner8.get_best_models(num_models=2)  # select the three three best models with corresponding hyperparameters\n",
    "y_prob_val_nn5 = best_model8[0].predict_proba(X_val)[:,1]\n",
    "y_prob_val_nn6 = best_model8[1].predict_proba(X_val)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's call our plot function to plot the ROC of each model compared to the recall\n",
    "plot_results(y_prob_val_nn5)\n",
    "plot_results(y_prob_val_nn6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.4 Hyperparameter Tuning 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiere Hyperparameter für die Lernrate und Batch-Größe\n",
    "def create_baseline9(hp):\n",
    "    model = Sequential()\n",
    "    for i in range(hp.Int('layers', 2, 15)):\n",
    "        model.add(Dense(\n",
    "            units=hp.Int('units_' + str(i), 10, 600, step=25), input_shape=(1523,),\n",
    "            activation=hp.Choice('act_' + str(i), ['relu', 'sigmoid'])\n",
    "        ))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    hp_batch_size = hp.Choice('batch_size', values=[32, 64, 128])\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=hp_learning_rate), metrics=[tf.keras.metrics.AUC(curve='ROC')])\n",
    "    return model\n",
    "\n",
    "\n",
    "tuner9 = RandomSearch(\n",
    "    create_baseline9,\n",
    "    objective=Objective('auc', direction='max'),\n",
    "    max_trials=5,\n",
    "    executions_per_trial=5,\n",
    "    directory='my_dir',\n",
    "    project_name='tuner3'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner9.search(X_train, y_train, epochs=5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner9.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are now interested in the best 3 models with the corresponding hyperparameters.\n",
    "best_model9 = tuner9.get_best_models(num_models=2)\n",
    "y_prob_val_nn7 = best_model9[0].predict_proba(X_val)[:,1]\n",
    "y_prob_val_nn8 = best_model9[1].predict_proba(X_val)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's call our plot function to plot the ROC of each model compared to the recall\n",
    "plot_results(y_prob_val_nn7)\n",
    "plot_results(y_prob_val_nn8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude we compar the roc curves of all models to choose the best one for out final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Erstellung und Anzeige von ROC-Kurven\n",
    "def plot_roc_curve(y_true, y_probs, label):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot erstellen\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{label} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "y_prob_list = [y_prob_val_rf1, y_prob_val_rf2, y_prob_val_rf3,\n",
    "               y_prob_val_nb, y_prob_val_dt1, y_prob_val_dt2,\n",
    "               y_prob_val_LR1, y_prob_val_lasso, y_prob_val_ridge,\n",
    "               y_prob_val_nn2, y_prob_val_nn3, y_prob_val_nn4, y_prob_val_nn5, y_prob_val_nn6, y_prob_val_nn7, y_prob_val_nn8]\n",
    "\n",
    "# Plot erstellen\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "for i, y_prob in enumerate(y_prob_list):\n",
    "    plot_roc_curve(y_val, y_prob, i)\n",
    "\n",
    "# Zufallsraten\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='Zufallsraten')\n",
    "\n",
    "# Achsenbeschriftungen und Titel hinzufügen\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison on Validation Set')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Anzeigen der Grafik\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the graph shows, many of the models perform very similar. We chose the random forest classifier with tuned hyperparameters for our final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = randomforestcv\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "#z_test = np.array(z_test.str[2:].astype(int))\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Extract the indices of false negatives\n",
    "fn_indices = np.where((y_test == True) & (y_pred == False))[0]\n",
    "\n",
    "# Use the indices to get the corresponding values from z_test\n",
    "fn_values = z_test[fn_indices]\n",
    "\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(fn_values, bins=[1, 2, 3, 4, 5], align='left', rwidth=0.8, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of True Values for False Negatives')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([1, 2, 3, 4], ['Death', 'Heavy Injury', 'Light Injury', 'Property Damage'])\n",
    "plt.show()\n",
    "\n",
    "plot_results(best_model.predict_proba(X_test)[:,1], y_true=y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, the graph above shows, that we predict about 200 false negatives, but most of them luckily only resultet in light personal injuries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Main-Dataset: https://data.stadt-zuerich.ch/dataset/sid_dav_strassenverkehrsunfallorte  startzeit, wahrscheinlich auch zeitumstellung\n",
    "- Weather data: https://data.stadt-zuerich.ch/dataset/ugz_meteodaten_stundenmittelwerte  startzeit, 10Uhr Sommer = 10Uhr i.R., 10Uhr Winter = 11Uhr i.R.\n",
    "- Traffic volume data: https://data.stadt-zuerich.ch/dataset/sid_dav_verkehrszaehlung_miv_od2031  startzeit, vorstellen: es fehlt 2Uhr, rückstellen: es gibt 2 Werte für 2Uhr\n",
    "- Streets network and speedlimit data: https://data.stadt-zuerich.ch/dataset/geo_signalisierte_geschwindigkeiten  \n",
    "- Pedestrian and bicycle volume data: https://data.stadt-zuerich.ch/dataset/ted_taz_verkehrszaehlungen_werte_fussgaenger_velo  startzeit, vorstellen: es fehlt 2Uhr, rückstellen: es gibt 2 Werte für 2Uhr\n",
    "- Trafic-areas: https://data.stadt-zuerich.ch/dataset/geo_verkehrszonen \n",
    "- Public transport stops: https://data.stadt-zuerich.ch/dataset/ktzh_haltestellen_des_oeffentlichen_verkehrs___ogd_\n",
    "- Public garages: https://www.stadt-zuerich.ch/geodaten/\n",
    "- Pedestrian and Bicycle network: https://data.stadt-zuerich.ch/dataset/geo_fuss__und_velowegnetz\n",
    "- Timeshift information: https://www.zeitumstellung.in\n",
    "- Holidays Information: https://www.stadt-zuerich.ch/portal/de/index/jobs/anstellungsbedingungen/ferien-urlaub-betriebsferientage/feiertage-betriebsferientage.html and https://www.feiertagskalender.ch/kalender.php?geo=3055&jahr=2026&hl=de&klasse=4&ft_id=20\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
